{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teapot Dome Recommender System and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorsys import hsv_to_rgb\n",
    "from random import uniform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define a few functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_splitter(dataframe, fraction, randomseed):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into train and test subsets\n",
    "    :param dataframe: Tops dataframe\n",
    "    :param fraction: The fraction of tops to use for validation\n",
    "    :param randomseed: The random seed for random sampling\n",
    "    :return: training and testing dataframes\n",
    "    \"\"\"\n",
    "    test = dataframe.sample(frac=fraction, random_state=randomseed)\n",
    "    test_idx = test.index.values\n",
    "    train = dataframe.drop(test_idx)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# ALS factorization modified from\n",
    "# https://github.com/mickeykedia/Matrix-Factorization-ALS/blob/master/ALS%20Python%20Implementation.py\n",
    "# here items are the formation and users are the well\n",
    "def runALS(A, R, n_factors, n_iterations, lambda_):\n",
    "    \"\"\"\n",
    "    Runs Alternating Least Squares algorithm in order to calculate matrix.\n",
    "    :param A: User-Item Matrix with ratings\n",
    "    :param R: User-Item Matrix with 1 if there is a rating or 0 if not\n",
    "    :param n_factors: How many factors each of user and item matrix will consider\n",
    "    :param n_iterations: How many times to run algorithm\n",
    "    :param lambda_: Regularization parameter\n",
    "    :return: users, items from ALS\n",
    "    \"\"\"\n",
    "    n, m = A.shape\n",
    "    np.random.seed(86)\n",
    "    Users = 5 * np.random.rand(n, n_factors,)\n",
    "    Items = 5 * np.random.rand(n_factors, m)\n",
    "\n",
    "    def get_error(A, Users, Items, R):\n",
    "        # This calculates the MSE of nonzero elements\n",
    "        return np.sum((R * (A - np.dot(Users, Items))) ** 2) / np.sum(R)\n",
    "\n",
    "    MAE_List = []\n",
    "\n",
    "    print(\"Starting Iterations\")\n",
    "    for iteration in range(n_iterations):\n",
    "        for i, Ri in enumerate(R):\n",
    "            Users[i] = np.linalg.solve(\n",
    "                np.dot(Items, np.dot(np.diag(Ri), Items.T))\n",
    "                + lambda_ * np.eye(n_factors),\n",
    "                np.dot(Items, np.dot(np.diag(Ri), A[i].T)),\n",
    "            ).T\n",
    "        for j, Rj in enumerate(R.T):\n",
    "            Items[:, j] = np.linalg.solve(\n",
    "                np.dot(Users.T, np.dot(np.diag(Rj), Users))\n",
    "                + lambda_ * np.eye(n_factors),\n",
    "                np.dot(Users.T, np.dot(np.diag(Rj), A[:, j])),\n",
    "            )\n",
    "        MAE_List.append(get_error(A, Users, Items, R))\n",
    "    return Users, Items\n",
    "\n",
    "\n",
    "def cross_validation(dataframe, random_seed, latent_vectors, n_iters, reg):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into 4 different folds for cross validation\n",
    "    :param dataframe: Tops dataframe\n",
    "    :param random_seed: The random seed for random sampling\n",
    "    :param latent_vectors: Number of latent vectors for matrix factorization\n",
    "    :param n_iters: Number of iterations to run ALS\n",
    "    :param reg: Lamda regularization value\n",
    "    :return: dataframe with predictions and MAE error\n",
    "    \"\"\"\n",
    "    full = []\n",
    "    block_1 = dataframe.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops2 = dataframe.drop(block_1)\n",
    "    block_2 = tops2.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops3 = tops2.drop(block_2)\n",
    "    block_3 = tops3.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops4 = tops3.drop(block_3)\n",
    "    block_4 = tops4.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    blocks = [block_1, block_2, block_3, block_4]\n",
    "    CV_MAE = []\n",
    "    bk = 1\n",
    "    for block in blocks:\n",
    "        print(f\"Starting Block {bk}\")\n",
    "        validate = dataframe.loc[block]\n",
    "        main_group = dataframe.drop(block)\n",
    "        print(f\"Validating on {block.shape[0]} tops\")\n",
    "        D_df = main_group.pivot_table(\"SS\", \"Formation\", \"API\").fillna(\n",
    "            0\n",
    "        )  # pivot table to move into sparse matrix land\n",
    "        R = D_df.values\n",
    "        A = binarize(R)\n",
    "\n",
    "        U, Vt = runALS(R, A, latent_vectors, n_iters, reg)\n",
    "\n",
    "        recommendations = np.dot(U, Vt)  # get the recommendations\n",
    "\n",
    "        recsys = pd.DataFrame(\n",
    "            data=recommendations[0:, 0:], index=D_df.index, columns=D_df.columns\n",
    "        )  # results\n",
    "\n",
    "        newDF = recsys.T\n",
    "        newDF.reset_index(inplace=True)\n",
    "\n",
    "        flat_preds = pd.DataFrame(recsys.unstack()).reset_index()\n",
    "\n",
    "        new_df = pd.merge(\n",
    "            validate,\n",
    "            flat_preds,\n",
    "            how=\"left\",\n",
    "            left_on=[\"API\", \"Formation\"],\n",
    "            right_on=[\"API\", \"Formation\"],\n",
    "        )\n",
    "\n",
    "        new_df.rename(columns={0: \"SS_pred\"}, inplace=True)\n",
    "\n",
    "        cleanDF = new_df.dropna()\n",
    "\n",
    "        cleanDF[\"signed_error\"] = cleanDF[\"SS\"] - cleanDF[\"SS_pred\"]\n",
    "        cleanDF[\"Block\"] = [bk] * cleanDF.shape[0]\n",
    "        well_locs.rename(columns={\"API Number\": \"API\"}, inplace=True)\n",
    "\n",
    "        full.append(cleanDF.merge(well_locs[[\"Northing\", \"Easting\", \"API\"]], on=\"API\"))\n",
    "        CV_MAE.append(MAE(cleanDF.SS.values - ssmin, cleanDF.SS_pred.values - ssmin))\n",
    "        bk += 1\n",
    "    output = pd.concat(full)\n",
    "    return output\n",
    "\n",
    "\n",
    "def cross_validation_error(dataframe, random_seed, latent_vectors, n_iters, reg):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into 4 different folds for cross validation MAE and RMSE\n",
    "    :param dataframe: Tops dataframe\n",
    "    :param random_seed: The random seed for random sampling\n",
    "    :param latent_vectors: Number of latent vectors for matrix factorization\n",
    "    :param n_iters: Number of iterations to run ALS\n",
    "    :param reg: Lamda regularization value\n",
    "    :return: MAE and RMSE error values for each fold\n",
    "    \"\"\"\n",
    "    full = []\n",
    "    block_1 = dataframe.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops2 = dataframe.drop(block_1)\n",
    "    block_2 = tops2.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops3 = tops2.drop(block_2)\n",
    "    block_3 = tops3.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops4 = tops3.drop(block_3)\n",
    "    block_4 = tops4.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    blocks = [block_1, block_2, block_3, block_4]\n",
    "    CV_MAE = []\n",
    "    CV_MSE = []\n",
    "    bk = 1\n",
    "    for block in blocks:\n",
    "        print(f\"Starting Block {bk}\")\n",
    "        validate = dataframe.loc[block]\n",
    "        main_group = dataframe.drop(block)\n",
    "        print(f\"Validating on {block.shape[0]} tops\")\n",
    "        D_df = main_group.pivot_table(\"SS\", \"Formation\", \"API\").fillna(\n",
    "            0\n",
    "        )  # pivot table to move into sparse matrix land\n",
    "        R = D_df.values\n",
    "        A = binarize(R)\n",
    "\n",
    "        U, Vt = runALS(R, A, latent_vectors, n_iters, reg)\n",
    "\n",
    "        recommendations = np.dot(U, Vt)  # get the recommendations\n",
    "\n",
    "        recsys = pd.DataFrame(\n",
    "            data=recommendations[0:, 0:], index=D_df.index, columns=D_df.columns\n",
    "        )  # results\n",
    "\n",
    "        newDF = recsys.T\n",
    "        newDF.reset_index(inplace=True)\n",
    "\n",
    "        flat_preds = pd.DataFrame(recsys.unstack()).reset_index()\n",
    "\n",
    "        new_df = pd.merge(\n",
    "            validate,\n",
    "            flat_preds,\n",
    "            how=\"left\",\n",
    "            left_on=[\"API\", \"Formation\"],\n",
    "            right_on=[\"API\", \"Formation\"],\n",
    "        )\n",
    "\n",
    "        new_df.rename(columns={0: \"SS_pred\"}, inplace=True)\n",
    "\n",
    "        cleanDF = new_df.dropna()\n",
    "\n",
    "        cleanDF[\"signed_error\"] = cleanDF[\"SS\"] - cleanDF[\"SS_pred\"]\n",
    "        cleanDF[\"Block\"] = [bk] * cleanDF.shape[0]\n",
    "        well_locs.rename(columns={\"API Number\": \"API\"}, inplace=True)\n",
    "\n",
    "        full.append(cleanDF.merge(well_locs[[\"Northing\", \"Easting\", \"API\"]], on=\"API\"))\n",
    "        CV_MAE.append(MAE(cleanDF.SS.values - ssmin, cleanDF.SS_pred.values - ssmin))\n",
    "        CV_MSE.append(\n",
    "            np.sqrt(MSE(cleanDF.SS.values - ssmin, cleanDF.SS_pred.values - ssmin))\n",
    "        )\n",
    "\n",
    "        bk += 1\n",
    "    return CV_MAE, CV_MSE\n",
    "\n",
    "\n",
    "def cross_validation_wells(dataframe, random_seed, latent_vectors, n_iters, reg):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into 4 different folds for cross validation on each well\n",
    "    :param dataframe: Tops dataframe\n",
    "    :param random_seed: The random seed for random sampling\n",
    "    :param latent_vectors: Number of latent vectors for matrix factorization\n",
    "    :param n_iters: Number of iterations to run ALS\n",
    "    :param reg: Lamda regularization value\n",
    "    :return: dataframe with error for each wells predictions\n",
    "    \"\"\"\n",
    "    cv_wells = []\n",
    "    block_1 = dataframe.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops2 = dataframe.drop(block_1)\n",
    "    block_2 = tops2.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops3 = tops2.drop(block_2)\n",
    "    block_3 = tops3.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops4 = tops3.drop(block_3)\n",
    "    block_4 = tops4.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    blocks = [block_1, block_2, block_3, block_4]\n",
    "    f = 0\n",
    "    for block in blocks:\n",
    "        validate = dataframe.loc[block]\n",
    "        main_group = dataframe.drop(block)\n",
    "        print(f\"Validating on {block.shape[0]} tops\")\n",
    "        D_df = main_group.pivot_table(\"SS\", \"Formation\", \"API\").fillna(\n",
    "            0\n",
    "        )  # pivot table to move into sparse matrix land\n",
    "        R = D_df.values\n",
    "        A = binarize(R)\n",
    "\n",
    "        U, Vt = runALS(R, A, latent_vectors, n_iters, reg)\n",
    "\n",
    "        recommendations = np.dot(U, Vt)  # get the recommendations\n",
    "\n",
    "        recsys = pd.DataFrame(\n",
    "            data=recommendations[0:, 0:], index=D_df.index, columns=D_df.columns\n",
    "        )  # results\n",
    "\n",
    "        newDF = recsys.T\n",
    "        newDF.reset_index(inplace=True)\n",
    "\n",
    "        flat_preds = pd.DataFrame(recsys.unstack()).reset_index()\n",
    "\n",
    "        new_df = pd.merge(\n",
    "            validate,\n",
    "            flat_preds,\n",
    "            how=\"left\",\n",
    "            left_on=[\"API\", \"Formation\"],\n",
    "            right_on=[\"API\", \"Formation\"],\n",
    "        )\n",
    "\n",
    "        new_df.rename(columns={0: \"SS_pred\"}, inplace=True)\n",
    "\n",
    "        cleanDF = new_df.dropna()\n",
    "\n",
    "        cleanDF[\"signed_error\"] = cleanDF[\"SS\"] - cleanDF[\"SS_pred\"]\n",
    "        well_locs.rename(columns={\"API Number\": \"API\"}, inplace=True)\n",
    "\n",
    "        locationDF = cleanDF.merge(well_locs[[\"Northing\", \"Easting\", \"API\"]], on=\"API\")\n",
    "        aypi = []\n",
    "        well_mae = []\n",
    "        well_rmse = []\n",
    "        east = []\n",
    "        north = []\n",
    "        fold = []\n",
    "\n",
    "        print(f\"foldno is {f}\")\n",
    "        for well in locationDF.API.unique():\n",
    "            aypi.append(well)\n",
    "            well_mae.append(\n",
    "                locationDF[locationDF.API == well].signed_error.abs().mean()\n",
    "            )\n",
    "            well_rmse.append(\n",
    "                np.sqrt(\n",
    "                    MSE(\n",
    "                        locationDF[locationDF.API == well].SS,\n",
    "                        locationDF[locationDF.API == well].SS_pred,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            east.append(locationDF[locationDF.API == well].Easting.values[0])\n",
    "            north.append(locationDF[locationDF.API == well].Northing.values[0])\n",
    "            fold.append(f)\n",
    "        by_wellDF = pd.DataFrame(\n",
    "            {\n",
    "                \"API\": aypi,\n",
    "                \"Well_MAE\": well_mae,\n",
    "                \"well_rmse\": well_rmse,\n",
    "                \"Easting\": east,\n",
    "                \"Northing\": north,\n",
    "                \"foldno\": fold,\n",
    "            }\n",
    "        )\n",
    "        cv_wells.append(by_wellDF)\n",
    "        f += 1\n",
    "\n",
    "    return cv_wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read in the munged dataset and well locations. We rename a column, and standardize the TVDSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = pd.read_csv(\"teapot_clean.csv\", index_col=[0])  # read in the top data\n",
    "tops.rename(columns={\"TVDSS\": \"SS\"}, inplace=True)\n",
    "well_locs = pd.read_csv(r\"../Munging/TeapotDomeWellHeaders.csv\")\n",
    "ssmin = tops.SS.min()\n",
    "tops.SS = tops.SS - ssmin  # standardize the subsea values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run a quick QC, and split the data into test train subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops.dropna(inplace=True)\n",
    "training, testing = sample_splitter(tops, 0.2, 86)\n",
    "print(f\"Training size is {len(training)} tops, and test size is {len(testing)} tops\")\n",
    "# pivot table to move into sparse matrix land\n",
    "D_df_QC = training.pivot_table(\"SS\", \"Formation\", \"API\").fillna(0)\n",
    "R_QC = D_df_QC.values\n",
    "A_QC = binarize(R_QC)\n",
    "print(\n",
    "    f\"{round(((D_df_QC == 0).astype(int).sum().sum())/((D_df_QC == 0).astype(int).sum().sum()+(D_df_QC != 0).astype(int).sum().sum()),3)*100} percent of the tops are missing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "This section takes a while to run, could be made more efficient with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "grid_search = {}\n",
    "els = []\n",
    "nsits = []\n",
    "regulars = []\n",
    "for L in range(1, 11):\n",
    "\n",
    "    for n_it in range(10, 450, 10):\n",
    "        for reg in [0.001, 0.01, 0.1, 1, 10]:\n",
    "            grid_search[L, n_it, reg] = np.mean(\n",
    "                cross_validation_error(tops, 86, L, n_it, reg)[0]\n",
    "            )\n",
    "            els.append(L)\n",
    "            nsits.append(n_it)\n",
    "            regulars.append(reg)\n",
    "            print(L, n_it, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assign the optimal number of latent features, iterations, and lambda value. the second line sets it to the searched value from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you ran grid search\n",
    "# L, its, regs = min(grid_search, key=grid_search.get)\n",
    "L, its, regs = [2, 290, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L, its, regs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_DF = cross_validation(tops, 86, 2, 290, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding stratigraphy\n",
    "Here we add in the ordered stratigraphy so we can make sense of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_order = [\n",
    "    \"SSXS\",\n",
    "    \"SSXbase\",\n",
    "    \"SHNNu\",\n",
    "    \"SHNNl\",\n",
    "    \"SHNNbs\",\n",
    "    \"StBR\",\n",
    "    \"StFT\",\n",
    "    \"StGD\",\n",
    "    \"StAM\",\n",
    "    \"NBRRws\",\n",
    "    \"NBRRsg\",\n",
    "    \"CRLL\",\n",
    "    \"F1WC\",\n",
    "    \"F1WCBench1Base\",\n",
    "    \"F1WCBench2Top\",\n",
    "    \"F1WCBench2Base\",\n",
    "    \"F1WCBench3Top\",\n",
    "    \"F1WCbase\",\n",
    "    \"B1\",\n",
    "    \"F2WC\",\n",
    "    \"F2WCbase\",\n",
    "    \"F3WC\",\n",
    "    \"F3WCbase\",\n",
    "    \"MWRY\",\n",
    "    \"MDDY\",\n",
    "    \"THRM\",\n",
    "    \"DKOT\",\n",
    "    \"LKOT\",  # base cret\n",
    "    \"MRSN\",\n",
    "    \"SNDCu\",  # upper sundance\n",
    "    \"CNSP\",  # base jurassic canyon spring\n",
    "    \"CRMT\",  # Crow mountain, Chugwater Group/FM\n",
    "    \"ALCV\",  # Alcova ls, chugwater\n",
    "    \"RDPK\",  # base Triassic chugwater\n",
    "    \"ERVY\",  # goose egg member\n",
    "    \"FRLL\",  # goose egg\n",
    "    \"GLND\",  # goose egg\n",
    "    \"MNKT\",  # goose egg\n",
    "    \"OPCH\",  # goose egg Base Permian\n",
    "    \"A Sand\",  # Tensleep\n",
    "    \"B Dolo\",\n",
    "    \"B Sand\",\n",
    "    \"C1 Dolo\",\n",
    "    \"C1 Sand\",\n",
    "    \"C2 Dolo\",\n",
    "    \"C2 Sand\",\n",
    "    \"C3 Dolo\",\n",
    "    \"C3 Sand\",\n",
    "    \"C4 Dolo\",\n",
    "    \"C4 Sand\",\n",
    "    \"D Dolo\",\n",
    "    \"D Sand\",\n",
    "    \"E Dolo\",\n",
    "    \"AMSD\",  # base Pennsylvanian\n",
    "    \"MDSN\",  # base mississippian\n",
    "    \"PC\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we assign some colors. The original output from this cell is saved as `color_palette.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = []\n",
    "for i in range(len(strat_order)):\n",
    "    if i <= 29:  # cretaceous\n",
    "        h = uniform(83 / 360, 158 / 360)  # Select random green'ish hue from hue wheel\n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.2, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r, g, b])\n",
    "    elif 29 < i <= 32:  # Jurassic\n",
    "        h = uniform(138 / 360, 200 / 360)\n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r, g, b])\n",
    "\n",
    "    elif 32 < i <= 35:  # triassic\n",
    "        h = uniform(156 / 360, 180 / 360)\n",
    "        s = uniform(0.2, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r, g, b])\n",
    "\n",
    "    elif 35 < i <= 40:  # permian\n",
    "        h = uniform(190 / 360, 200 / 360)\n",
    "        s = uniform(0.2, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r, g, b])\n",
    "\n",
    "    elif 40 < i <= 55:  # Pennsylvanian\n",
    "        h = uniform(210 / 360, 230 / 360)\n",
    "        s = uniform(0.2, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r, g, b])\n",
    "\n",
    "    elif 55 < i <= 56:  # missippian\n",
    "        h = uniform(240 / 360, 260 / 360)\n",
    "        s = uniform(0.2, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r, g, b])\n",
    "\n",
    "    else:  # PC\n",
    "        h = uniform(300 / 360, 320 / 360)\n",
    "        s = uniform(0.2, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r, g, b])\n",
    "# colors_to_plot = dict(zip(strat_order, color_list))\n",
    "# uncomment if you want to create new colors, otherwise just load from csv\n",
    "# pd.DataFrame(colors_to_plot).to_csv('color_palette.csv', index=False)\n",
    "colors_to_plot = pd.read_csv(\"color_palette.csv\").to_dict(orient=\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for block in cv_DF[\"Block\"].unique():\n",
    "    locationDF = cv_DF[cv_DF[\"Block\"] == block]\n",
    "    mae_errors = []\n",
    "    n_holdout = []\n",
    "    formed = []\n",
    "    colored = []\n",
    "    stdev = []\n",
    "    n_train = []\n",
    "    rms_errors = []\n",
    "    for formation in strat_order:\n",
    "        form = locationDF[locationDF[\"Formation\"] == formation]\n",
    "        if form.SS.values.shape[0] > 0:\n",
    "            mae_errors.append(\n",
    "                round(MAE(form.SS.values - ssmin, form.SS_pred.values - ssmin), 1)\n",
    "            )\n",
    "            formed.append(formation)\n",
    "            colored.append(colors_to_plot[formation])\n",
    "            stdev.append(np.std(form.signed_error))\n",
    "            n_holdout.append(form.shape[0])\n",
    "            n_train.append(tops[tops.Formation == formation].shape[0] - form.shape[0])\n",
    "\n",
    "            rms_errors.append(\n",
    "                np.sqrt(MSE(form.SS.values - ssmin, form.SS_pred.values - ssmin))\n",
    "            )\n",
    "        else:\n",
    "            mae_errors.append(np.nan)\n",
    "            formed.append(formation)\n",
    "            colored.append(colors_to_plot[formation])\n",
    "            stdev.append(np.std(form.signed_error))\n",
    "            n_train.append(tops[tops.Formation == formation].shape[0] - form.shape[0])\n",
    "            n_holdout.append(form.shape[0])\n",
    "            rms_errors.append(np.nan)\n",
    "    table_1 = pd.DataFrame(\n",
    "        {\n",
    "            \"Formation\": formed,\n",
    "            \"n_train\": n_train,\n",
    "            \"n_holdout\": n_holdout,\n",
    "            \"MAE\": mae_errors,\n",
    "            \"RMSE\": rms_errors,\n",
    "            \"Std\": stdev,\n",
    "        }\n",
    "    )\n",
    "    nums = dict(zip(formed, n_holdout))\n",
    "    errers = dict(zip(formed, mae_errors))\n",
    "    val = dict(zip(formed, n_holdout))\n",
    "    # table_1.to_csv('Table_1_block_'+str(block)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file outputs of the cell above are manually combined into `errors.csv` and read in to the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.read_csv(\"errors.csv\")\n",
    "grid = plt.GridSpec(2, 1, wspace=0.4, hspace=0.1)\n",
    "fig = plt.figure(figsize=(2.5, 5))\n",
    "upper = plt.subplot(grid[0, :])\n",
    "for formation in strat_order:\n",
    "    subset = errors[errors.Formation == formation]\n",
    "    upper.scatter(\n",
    "        subset[\"n_holdout\"] + subset[\"n_train\"],\n",
    "        subset[\"MAE\"] * 3.28084,\n",
    "        color=colors_to_plot[str(formation)],\n",
    "        alpha=0.75,\n",
    "        s=15,\n",
    "    )\n",
    "    upper.scatter(\n",
    "        subset[\"n_holdout.1\"] + subset[\"n_train.1\"],\n",
    "        subset[\"MAE.1\"] * 3.28084,\n",
    "        color=colors_to_plot[str(formation)],\n",
    "        alpha=0.75,\n",
    "        s=15,\n",
    "    )\n",
    "    upper.scatter(\n",
    "        subset[\"n_holdout.2\"] + subset[\"n_train.2\"],\n",
    "        subset[\"MAE.2\"] * 3.28084,\n",
    "        color=colors_to_plot[str(formation)],\n",
    "        alpha=0.75,\n",
    "        s=15,\n",
    "    )\n",
    "    up = upper.scatter(\n",
    "        subset[\"n_holdout.3\"] + subset[\"n_train.3\"],\n",
    "        subset[\"MAE.3\"] * 3.28084,\n",
    "        color=colors_to_plot[str(formation)],\n",
    "        alpha=0.75,\n",
    "        s=15,\n",
    "    )\n",
    "upper.semilogx()\n",
    "upper.semilogy()\n",
    "upper.set_ylabel(\"MAE (ft)\", fontsize=6)\n",
    "upper.set_title(\"Error and Number of Picks\", fontsize=6)\n",
    "\n",
    "upper.set_xticklabels(\n",
    "    [10 ** -2, 10 ** -1, 10 ** 1, 10 ** 3], fontsize=6,\n",
    ")\n",
    "upper.set_yticklabels([10 ^ -1, 10 ** 0, 10 ** 1, 10 ** 2, 10 ** 3], fontsize=6)\n",
    "upper.set_xlim(0.1, 5000)\n",
    "upper.set_ylim(1, 2000)\n",
    "\n",
    "\n",
    "lower = plt.subplot(grid[1, :])\n",
    "\n",
    "for formation in strat_order:\n",
    "    subset = errors[errors.Formation == formation]\n",
    "    lower.scatter(\n",
    "        subset[\"n_holdout\"] + subset[\"n_train\"],\n",
    "        subset[\"RMSE\"] * 3.28084,\n",
    "        color=colors_to_plot[str(formation)],\n",
    "        alpha=0.75,\n",
    "        s=15,\n",
    "    )\n",
    "    lower.scatter(\n",
    "        subset[\"n_holdout.1\"] + subset[\"n_train.1\"],\n",
    "        subset[\"RMSE.1\"] * 3.28084,\n",
    "        color=colors_to_plot[str(formation)],\n",
    "        alpha=0.75,\n",
    "        s=15,\n",
    "    )\n",
    "    lower.scatter(\n",
    "        subset[\"n_holdout.2\"] + subset[\"n_train.2\"],\n",
    "        subset[\"RMSE.2\"] * 3.28084,\n",
    "        color=colors_to_plot[str(formation)],\n",
    "        alpha=0.75,\n",
    "        s=15,\n",
    "    )\n",
    "    lo = lower.scatter(\n",
    "        subset[\"n_holdout.3\"] + subset[\"n_train.3\"],\n",
    "        subset[\"RMSE.3\"] * 3.28084,\n",
    "        color=colors_to_plot[str(formation)],\n",
    "        alpha=0.75,\n",
    "        s=15,\n",
    "    )\n",
    "lower.semilogx()\n",
    "lower.semilogy()\n",
    "lower.set_xlabel(\"Number of top picks of each formation\", fontsize=6)\n",
    "lower.set_ylabel(\"RMSE (ft)\", fontsize=6)\n",
    "lower.set_xticklabels(\n",
    "    [10 ** -2, 10 ** -1, 10 ** 1, 10 ** 3], fontsize=6,\n",
    ")\n",
    "lower.set_yticklabels([10 ^ -1, 10 ** 0, 10 ** 1, 10 ** 2, 10 ** 3], fontsize=6)\n",
    "lower.set_xlim(0.1, 5000)\n",
    "lower.set_ylim(1, 2000)\n",
    "# plt.savefig('RMSE_formation.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training size and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "big_de_error = []\n",
    "for ran in range(100):\n",
    "    de_error = []\n",
    "    for i in np.arange(0.99, 0, -0.1):\n",
    "        training, testing = sample_splitter(tops, i, ran)\n",
    "        print(f\"Training size is {len(training)} tops, and test size is {len(testing)} tops\")\n",
    "        D_df = training.pivot_table(\"SS\", \"Formation\", \"API\").fillna(\n",
    "            0\n",
    "        )  # pivot table to move into sparse matrix land\n",
    "        R = D_df.values\n",
    "\n",
    "        A = binarize(R)\n",
    "        U, Vt = runALS(R, A, 3, 20, 0.1)\n",
    "        recommendations = np.dot(U, Vt)  # get the recommendations\n",
    "        recsys = pd.DataFrame(\n",
    "            data=recommendations[0:, 0:], index=D_df.index, columns=D_df.columns\n",
    "        )  # results\n",
    "        newDF = recsys.T\n",
    "        newDF.reset_index(inplace=True)\n",
    "        # test = newDF[[\"FORT UNION\", \"API\"]]\n",
    "        flat_preds = pd.DataFrame(recsys.unstack()).reset_index()\n",
    "        new_df = pd.merge(\n",
    "            testing,\n",
    "            flat_preds,\n",
    "            how=\"left\",\n",
    "            left_on=[\"API\", \"Formation\"],\n",
    "            right_on=[\"API\", \"Formation\"],\n",
    "        )\n",
    "        new_df.rename(columns={0: \"SS_pred\"}, inplace=True)\n",
    "        cleanDF = new_df.dropna()\n",
    "        cleanDF[\"signed_error\"] = cleanDF[\"SS\"] - cleanDF[\"SS_pred\"]\n",
    "        de_error.append(\n",
    "            np.sqrt(MSE(cleanDF.SS.values - ssmin, cleanDF.SS_pred.values - ssmin))\n",
    "        )\n",
    "\n",
    "        locationDF = cleanDF.merge(well_locs[[\"Northing\", \"Easting\", \"API\"]], on=\"API\")\n",
    "    big_de_error.append(de_error)\n",
    "\n",
    "\n",
    "for i in range(len(big_de_error)):\n",
    "\n",
    "    plt.plot(\n",
    "        np.arange(0.01, 1, 0.1),\n",
    "        big_de_error[i],\n",
    "        c=\"k\",\n",
    "        marker=\"o\",\n",
    "        markersize=1,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    plt.ylim(1, 10000)\n",
    "    plt.semilogy()\n",
    "    plt.ylabel(\"Root Mean Squared Error (ft)\")\n",
    "    plt.xlabel(\"Percent of dataset used to train\")\n",
    "    plt.title(\"Root Mean Squared Error for 100 random train-test splits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error by well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDF = pd.concat(cross_validation_wells(tops, 86, 2, 290, 0.1))\n",
    "# masterDF.to_csv('error_map.csv')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
