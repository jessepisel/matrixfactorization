{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on the Mannville Group Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import lasio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.preprocessing import binarize\n",
    "from colorsys import hsv_to_rgb\n",
    "from random import randint, uniform\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "well LAS files from <https://dataunderground.org/dataset/athabasca> \n",
    "\n",
    "These LAS files are pulled to get the datum elevations, well names and convert all tops to TVDSS. You must download the data from the link above and set the path in `get_well_data` to the path of the LAS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_well_data(path):\n",
    "    \"\"\"\n",
    "    This function gets the Mannville data from the LAS files\n",
    "    Including, UWI, datum, ground elevation, reference elevation, and elevation units\n",
    "    :param path: path to the unzipped well LAS files\n",
    "    :return: dataframe with well data\n",
    "    \"\"\"\n",
    "    wells = glob.glob(PATH)\n",
    "    uwi =[]\n",
    "    datum = []\n",
    "    ground = []\n",
    "    eref = []\n",
    "    units = []\n",
    "    failure = 0\n",
    "    for well in wells[0:]:\n",
    "        curve = lasio.read(well)\n",
    "        try:\n",
    "            da = curve.params['DATM'].value\n",
    "            gl = curve.params['GL'].value\n",
    "            u = curve.well['UWI'].value\n",
    "            units.append(curve.params['GL'].unit)\n",
    "            eref.append(curve.params['EREF'].value)\n",
    "            uwi.append(u.replace('W400', 'W4/0'))\n",
    "            datum.append(da)\n",
    "            ground.append(gl)\n",
    "        except:\n",
    "            failure +=1\n",
    "            print(well)\n",
    "    print(str(failure) + ' logs failed to parse')\n",
    "    well_dict = pd.read_csv(r'mann_well_dict.csv')\n",
    "    dataframe = pd.DataFrame({\"UWI\":uwi, \"DATUM\":datum, \"GL\":ground, 'EREF':eref, 'UNIT':units})\n",
    "    dataframe['datum'] = np.where(dataframe.UNIT=='F', dataframe.EREF*0.304, dataframe.EREF)\n",
    "    # drop wells with no surface datum\n",
    "    bad_datums = ['00/07-11-082-07W4/0',\n",
    "        '00/10-08-095-21W4/0',\n",
    "        '00/11-29-094-21W4/0',\n",
    "        'AA/05-01-096-11W4/0',\n",
    "        'AA/06-09-095-10W4/0',\n",
    "        'AA/06-31-096-10W4/0',\n",
    "        'AA/08-25-096-13W4/0',\n",
    "        'AA/10-33-097-06W4/0',\n",
    "        'AA/15-36-096-11W4/0',\n",
    "        'AB/07-12-093-10W4/0',\n",
    "        'AB/10-18-096-10W4/0']\n",
    "    bad_wells = dataframe[dataframe['UWI'].isin(bad_datums)].index.values\n",
    "    dataframe.drop(bad_wells, inplace=True)\n",
    "    dataframe.to_csv('mann_dirty_LAS.csv')\n",
    "    return dataframe\n",
    "\n",
    "def clean_well_data():\n",
    "    \"\"\"\n",
    "    This function cleans the output from get_well_data and saves it to a csv\n",
    "    :param none:\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    well_dict = pd.read_csv(r'mann_well_dict.csv')\n",
    "    dataframe = pd.read_csv('mann_dirty_LAS.csv')\n",
    "    new_df = pd.merge(dataframe, well_dict,  how='left', left_on=['UWI'], right_on = ['UWI'])\n",
    "    new_df.dropna(inplace=True)\n",
    "    tops = pd.read_csv(r\"mannvillegrp_picks.csv\") #read in the top data\n",
    "    tops.rename(columns={'Pick':'MD'}, inplace=True)\n",
    "    new_df = pd.merge(tops, new_df,  how='left', left_on=['SitID'], right_on = ['SitID'])\n",
    "    not_nullDF = new_df.loc[new_df['UWI'].notnull()]\n",
    "    not_nullDF['TVDSS'] = (not_nullDF.datum-not_nullDF.MD).values\n",
    "    not_nullDF.to_csv('mannville_cleaned.csv')\n",
    "\n",
    "################################################    \n",
    "\n",
    "def sample_splitter(dataframe, fraction, randomseed):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into train and test subsets\n",
    "    :param dataframe: Tops dataframe\n",
    "    :param fraction: The fraction of tops to use for validation\n",
    "    :param randomseed: The random seed for random sampling\n",
    "    :return: training and testing dataframes\n",
    "    \"\"\"\n",
    "    test = dataframe.sample(frac=fraction, random_state=randomseed)\n",
    "    test_idx = test.index.values\n",
    "    train = dataframe.drop(test_idx)\n",
    "    return train, test\n",
    "\n",
    "# ALS factorization modified from\n",
    "# https://github.com/mickeykedia/Matrix-Factorization-ALS/blob/master/ALS%20Python%20Implementation.py\n",
    "# here items are the formation and users are the well\n",
    "def runALS(A, R, n_factors, n_iterations, lambda_):\n",
    "    \"\"\"\n",
    "    Runs Alternating Least Squares algorithm in order to calculate matrix.\n",
    "    :param A: User-Item Matrix with ratings\n",
    "    :param R: User-Item Matrix with 1 if there is a rating or 0 if not\n",
    "    :param n_factors: How many factors each of user and item matrix will consider\n",
    "    :param n_iterations: How many times to run algorithm\n",
    "    :param lambda_: Regularization parameter\n",
    "    :return: users, items from ALS\n",
    "    \"\"\"\n",
    "    n, m = A.shape\n",
    "    np.random.seed(86)\n",
    "    Users = 5 * np.random.rand(n, n_factors,)\n",
    "    Items = 5 * np.random.rand(n_factors, m)\n",
    "\n",
    "    def get_error(A, Users, Items, R):\n",
    "        # This calculates the MSE of nonzero elements\n",
    "        return np.sum((R * (A - np.dot(Users, Items))) ** 2) / np.sum(R)\n",
    "\n",
    "    MAE_List = []\n",
    "\n",
    "    print(\"Starting Iterations\")\n",
    "    for iteration in range(n_iterations):\n",
    "        for i, Ri in enumerate(R):\n",
    "            Users[i] = np.linalg.solve(\n",
    "                np.dot(Items, np.dot(np.diag(Ri), Items.T))\n",
    "                + lambda_ * np.eye(n_factors),\n",
    "                np.dot(Items, np.dot(np.diag(Ri), A[i].T)),\n",
    "            ).T\n",
    "        for j, Rj in enumerate(R.T):\n",
    "            Items[:, j] = np.linalg.solve(\n",
    "                np.dot(Users.T, np.dot(np.diag(Rj), Users))\n",
    "                + lambda_ * np.eye(n_factors),\n",
    "                np.dot(Users.T, np.dot(np.diag(Rj), A[:, j])),\n",
    "            )\n",
    "        MAE_List.append(get_error(A, Users, Items, R))\n",
    "    return Users, Items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_error(dataframe, random_seed, latent_vectors, n_iters, reg):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into 4 different folds for cross validation MAE and RMSE\n",
    "    :param dataframe: Tops dataframe\n",
    "    :param random_seed: The random seed for random sampling\n",
    "    :param latent_vectors: Number of latent vectors for matrix factorization\n",
    "    :param n_iters: Number of iterations to run ALS\n",
    "    :param reg: Lamda regularization value\n",
    "    :return: MAE and RMSE error values for each fold\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    block_1  = dataframe.sample(n=dataframe.shape[0]//4, random_state=random_seed).index.values\n",
    "    tops2 = dataframe.drop(block_1)\n",
    "    block_2 = tops2.sample(n=dataframe.shape[0]//4, random_state=random_seed).index.values\n",
    "    tops3 = tops2.drop(block_2)\n",
    "    block_3 = tops3.sample(n=dataframe.shape[0]//4, random_state=random_seed).index.values\n",
    "    tops4 = tops3.drop(block_3)\n",
    "    block_4 = tops4.sample(n=dataframe.shape[0]//4, random_state=random_seed).index.values\n",
    "    blocks = [block_1, block_2, block_3, block_4]\n",
    "    CV_MAE = []\n",
    "    CV_MSE = []\n",
    "    for block in blocks:\n",
    "        validate = dataframe.loc[block]\n",
    "        main_group = dataframe.drop(block)\n",
    "        print(f'Validating on {block.shape[0]} tops')\n",
    "        D_df = main_group.pivot_table(\"TVDSS\", \"Formation\", \"SitID\").fillna(0)#pivot table to move into sparse matrix land\n",
    "        R = D_df.values\n",
    "        A = binarize(R) \n",
    "\n",
    "        \n",
    "        U, Vt = runALS(R, A, latent_vectors, n_iters, reg)\n",
    "\n",
    "        recommendations = np.dot(U, Vt) #get the recommendations\n",
    "\n",
    "        recsys = pd.DataFrame(\n",
    "            data=recommendations[0:, 0:], index=D_df.index, columns=D_df.columns\n",
    "        ) #results\n",
    "\n",
    "        newDF = recsys.T\n",
    "        newDF.reset_index(inplace=True)\n",
    "\n",
    "        flat_preds = pd.DataFrame(recsys.unstack()).reset_index()\n",
    "\n",
    "        new_df = pd.merge(validate, flat_preds,  how='left', left_on=['SitID','Formation'], right_on = ['SitID','Formation'])\n",
    "\n",
    "        new_df.rename(columns={0:'SS_pred'}, inplace=True)\n",
    "\n",
    "        cleanDF = new_df.dropna()\n",
    "\n",
    "        cleanDF['signed_error'] = (cleanDF['TVDSS'] - cleanDF['SS_pred'])\n",
    "\n",
    "        CV_MAE.append(MAE(cleanDF.TVDSS.values, cleanDF.SS_pred.values))\n",
    "        CV_MSE.append(np.sqrt(MSE(cleanDF.TVDSS.values, cleanDF.SS_pred.values)))\n",
    "\n",
    "    return CV_MAE, CV_MSE\n",
    "\n",
    "def cross_validation(dataframe, random_seed, latent_vectors, n_iters, reg):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into 4 different folds for cross validation\n",
    "    :param dataframe: Tops dataframe\n",
    "    :param random_seed: The random seed for random sampling\n",
    "    :param latent_vectors: Number of latent vectors for matrix factorization\n",
    "    :param n_iters: Number of iterations to run ALS\n",
    "    :param reg: Lamda regularization value\n",
    "    :return: dataframe with predictions and MAE error\n",
    "    \"\"\"\n",
    "    full = []\n",
    "    block_1 = dataframe.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops2 = dataframe.drop(block_1)\n",
    "    block_2 = tops2.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops3 = tops2.drop(block_2)\n",
    "    block_3 = tops3.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops4 = tops3.drop(block_3)\n",
    "    block_4 = tops4.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    blocks = [block_1, block_2, block_3, block_4]\n",
    "    CV_MAE = []\n",
    "    bk = 1\n",
    "    for block in blocks:\n",
    "        print(f'Starting Block {bk}')\n",
    "        validate = dataframe.loc[block]\n",
    "        main_group = dataframe.drop(block)\n",
    "        print(f'Validating on {block.shape[0]} tops')\n",
    "        D_df = main_group.pivot_table(\"TVDSS\", \"Formation\", \"SitID\").fillna(0)#pivot table to move into sparse matrix land\n",
    "        R = D_df.values\n",
    "        A = binarize(R) \n",
    "\n",
    "        \n",
    "        U, Vt = runALS(R, A, latent_vectors, n_iters, reg)\n",
    "\n",
    "        recommendations = np.dot(U, Vt) #get the recommendations\n",
    "\n",
    "        recsys = pd.DataFrame(\n",
    "            data=recommendations[0:, 0:], index=D_df.index, columns=D_df.columns\n",
    "        ) #results\n",
    "\n",
    "        newDF = recsys.T\n",
    "        newDF.reset_index(inplace=True)\n",
    "\n",
    "        flat_preds = pd.DataFrame(recsys.unstack()).reset_index()\n",
    "\n",
    "        newDF = recsys.T\n",
    "        newDF.reset_index(inplace=True)\n",
    "\n",
    "        flat_preds = pd.DataFrame(recsys.unstack()).reset_index()\n",
    "\n",
    "        new_df = pd.merge(validate, flat_preds,  how='left', left_on=['SitID','Formation'], right_on = ['SitID','Formation'])\n",
    "\n",
    "        new_df.rename(columns={0:'SS_pred'}, inplace=True)\n",
    "\n",
    "        cleanDF = new_df.dropna()\n",
    "\n",
    "        cleanDF['signed_error'] = (cleanDF['TVDSS'] - cleanDF['SS_pred'])\n",
    "        cleanDF['Block'] = [bk]*cleanDF.shape[0]\n",
    "        well_locs = pd.read_csv(r'well_lat_lng.csv')\n",
    "        full.append(cleanDF.merge(well_locs[['lat', 'lng', 'SitID']], on='SitID'))\n",
    "        CV_MAE.append(MAE(cleanDF.TVDSS.values, cleanDF.SS_pred.values))\n",
    "        bk += 1\n",
    "    output = pd.concat(full)\n",
    "    return output\n",
    "\n",
    "def cross_validation_wells(dataframe, random_seed, latent_vectors, n_iters, reg):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into 4 different folds for cross validation on each well\n",
    "    :param dataframe: Tops dataframe\n",
    "    :param random_seed: The random seed for random sampling\n",
    "    :param latent_vectors: Number of latent vectors for matrix factorization\n",
    "    :param n_iters: Number of iterations to run ALS\n",
    "    :param reg: Lamda regularization value\n",
    "    :return: dataframe with error for each wells predictions\n",
    "    \"\"\"\n",
    "    cv_wells = []\n",
    "    block_1 = dataframe.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops2 = dataframe.drop(block_1)\n",
    "    block_2 = tops2.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops3 = tops2.drop(block_2)\n",
    "    block_3 = tops3.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    tops4 = tops3.drop(block_3)\n",
    "    block_4 = tops4.sample(\n",
    "        n=dataframe.shape[0] // 4, random_state=random_seed\n",
    "    ).index.values\n",
    "    blocks = [block_1, block_2, block_3, block_4]\n",
    "    f = 0\n",
    "    for block in blocks:\n",
    "        validate = dataframe.loc[block]\n",
    "        main_group = dataframe.drop(block)\n",
    "        print(f'Validating on {block.shape[0]} tops')\n",
    "        D_df = main_group.pivot_table(\"TVDSS\", \"Formation\", \"SitID\").fillna(0)#pivot table to move into sparse matrix land\n",
    "        R = D_df.values\n",
    "        A = binarize(R) \n",
    "\n",
    "        \n",
    "        U, Vt = runALS(R, A, latent_vectors, n_iters, reg)\n",
    "\n",
    "        recommendations = np.dot(U, Vt) #get the recommendations\n",
    "\n",
    "        recsys = pd.DataFrame(\n",
    "            data=recommendations[0:, 0:], index=D_df.index, columns=D_df.columns\n",
    "        ) #results\n",
    "\n",
    "        newDF = recsys.T\n",
    "        newDF.reset_index(inplace=True)\n",
    "\n",
    "        flat_preds = pd.DataFrame(recsys.unstack()).reset_index()\n",
    "\n",
    "        new_df = pd.merge(validate, flat_preds,  how='left', left_on=['SitID','Formation'], right_on = ['SitID','Formation'])\n",
    "\n",
    "        new_df.rename(columns={0:'SS_pred'}, inplace=True)\n",
    "\n",
    "        cleanDF = new_df.dropna()\n",
    "\n",
    "        cleanDF['signed_error'] = (cleanDF['TVDSS'] - cleanDF['SS_pred'])\n",
    "        well_locs = pd.read_csv(r'well_lat_lng.csv')\n",
    "        locationDF = cleanDF.merge(well_locs[['lat', 'lng', 'SitID']], on='SitID')\n",
    "        aypi = []\n",
    "        well_mae = []\n",
    "        well_rmse = []\n",
    "        east = []\n",
    "        north = []\n",
    "        fold = []\n",
    "        \n",
    "        print(f'foldno is {f}')\n",
    "        for well in locationDF.SitID.unique():\n",
    "            aypi.append(well)\n",
    "            well_mae.append(locationDF[locationDF.SitID == well].signed_error.abs().mean())\n",
    "            well_rmse.append(np.sqrt(MSE(locationDF[locationDF.SitID == well].TVDSS,\n",
    "                                         locationDF[locationDF.SitID == well].SS_pred)))\n",
    "            east.append(locationDF[locationDF.SitID == well].lng.values[0])\n",
    "            north.append(locationDF[locationDF.SitID == well].lat.values[0])\n",
    "            fold.append(f)\n",
    "        by_wellDF = pd.DataFrame({'SitID':aypi, 'Well_MAE':well_mae, 'well_rmse':well_rmse, 'Longitude':east, 'Latitude':north,\n",
    "                                 'foldno':fold})\n",
    "        cv_wells.append(by_wellDF)\n",
    "        f+=1\n",
    "    return cv_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '*.las' # path to the las files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty = get_well_data(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_well_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are the predictions, above is cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = pd.read_csv(r\"mannville_cleaned.csv\", index_col=[0]) #read in the top data\n",
    "print(tops.shape)\n",
    "tops.dropna(inplace=True)\n",
    "print(tops.shape)\n",
    "tops = tops[tops.Quality >=0]\n",
    "print(tops.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = sample_splitter(tops, 0.1, 86)\n",
    "\n",
    "print(f'Training size is {len(training)} tops, and test size is {len(testing)} tops')\n",
    "\n",
    "QC_D_df = training.pivot_table(\"TVDSS\", \"Formation\", \"SitID\").fillna(0)#pivot table to move into sparse matrix land\n",
    "QC_R = QC_D_df.values\n",
    "QC_A = binarize(QC_R) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{round(((QC_D_df == 0).astype(int).sum().sum())/((QC_D_df == 0).astype(int).sum().sum()+(QC_D_df != 0).astype(int).sum().sum()),3)*100} percent of the tops are missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(tops.UWI.unique())} wells and {len(tops.Formation.unique())} tops')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "grid_search = {}\n",
    "els = []\n",
    "nsits = []\n",
    "regulars = []\n",
    "for L in range(1, 11):\n",
    "    for n_it in range(10, 450, 10):\n",
    "        for reg in [0.001, 0.01, 0.1, 1, 10]:\n",
    "            grid_search[L, n_it, reg] = np.mean(\n",
    "                cross_validation_error(tops, 86, L, n_it, reg)\n",
    "            )\n",
    "            els.append(L)\n",
    "            nsits.append(n_it)\n",
    "            regulars.append(reg)\n",
    "            print(L, n_it, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L, its, lbda = min(grid_search, key=grid_search.get)\n",
    "L, its, lbda = [3, 100, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, rmse = cross_validation_error(tops, 86, L, its, lbda)\n",
    "print(mae, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mae), np.mean(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_DF = cross_validation(tops, 86, L, its, lbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding stratigraphy\n",
    "Here we add in the ordered stratigraphy so we can make sense of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_order = [1000,2000,3000,4000,5000,6000,7000,8000,9000,9500,10000,11000,12000,13000,14000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we assign some colors. The original output from this cell is saved as `mann_color_palette.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = []\n",
    "for i in range(len(strat_order)):\n",
    "    if i ==0: #mannville\n",
    "        h = uniform(23/360, 33/360) # Select random green'ish hue from hue wheel\n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.2, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "    elif 0<i<=4: #t61 to t31\n",
    "        h = uniform(83/360, 158/360) \n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "\n",
    "    elif 4<i<=5: #clw_wab\n",
    "        h = uniform(23/360, 33/360) \n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "\n",
    "    elif 5<i<= 6: #t21\n",
    "        h = uniform(83/360, 158/360) \n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "\n",
    "    elif 6<i<= 7: #e20\n",
    "        h = uniform(23/360, 33/360) \n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "\n",
    "    elif 7<i<= 8: #t15\n",
    "        h = uniform(83/360, 158/360) \n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "        \n",
    "    elif 8<i<= 9: #e14\n",
    "        h = uniform(23/360, 33/360) \n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "        \n",
    "    elif 9<i<= 10: #t11\n",
    "        h = uniform(83/360, 158/360) \n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "    \n",
    "    elif 10<i<= 11: #t10.5\n",
    "        h = uniform(83/360, 158/360) \n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.3, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "        \n",
    "    elif 11<i<= 12: #e10\n",
    "        h = uniform(23/360, 33/360) \n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.2, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "        \n",
    "    elif 12<i<= 13: #mcmurray\n",
    "        h = uniform(50/360, 60/360) \n",
    "        s = uniform(0.8, 1)\n",
    "        v = uniform(0.2, 1)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "        \n",
    "    else: #PZ\n",
    "        h = uniform(300/360, 320/360) \n",
    "        s = uniform(0.2, 1)\n",
    "        v = uniform(0.3, 0.5)\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        color_list.append([r,g,b])\n",
    "\n",
    "#colors_to_plot = dict(zip(strat_order, color_list))\n",
    "#pd.DataFrame(colors_to_plot).to_csv('mann_color_palette.csv', index=False)\n",
    "colors_to_plot =  pd.read_csv('mann_color_palette.csv').to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in cv_DF['Block'].unique():\n",
    "    locationDF = cv_DF[cv_DF['Block'] == block]\n",
    "    mae_errors = []\n",
    "    n_holdout = []\n",
    "    formed = []\n",
    "    colored = []\n",
    "    stdev = []\n",
    "    n_train = []\n",
    "    rms_errors = []\n",
    "    for formation in strat_order:\n",
    "        form = locationDF[locationDF['Formation'] == formation]\n",
    "        if form.TVDSS.values.shape[0] > 0:\n",
    "            mae_errors.append(round(MAE(form.TVDSS.values, form.SS_pred.values),1))\n",
    "            formed.append(formation)\n",
    "            stdev.append(np.std(form.signed_error))\n",
    "            n_holdout.append(form.shape[0])\n",
    "            n_train.append(tops[tops.Formation == formation].shape[0]-form.shape[0])\n",
    "\n",
    "            rms_errors.append(np.sqrt(MSE(form.TVDSS.values, form.SS_pred.values)))\n",
    "        else:\n",
    "            mae_errors.append(np.nan)\n",
    "            formed.append(formation)\n",
    "            stdev.append(np.std(form.signed_error))\n",
    "            n_train.append(tops[tops.Formation == formation].shape[0]-form.shape[0])\n",
    "            n_holdout.append(form.shape[0])\n",
    "            rms_errors.append(np.nan)\n",
    "    table_1 = pd.DataFrame({'Formation':formed, 'n_train': n_train, 'n_holdout':n_holdout, 'MAE':mae_errors,\n",
    "                 'RMSE':rms_errors, 'Std':stdev})\n",
    "    nums = dict(zip(formed, n_holdout))\n",
    "    errers = dict(zip(formed, mae_errors))\n",
    "    val = dict(zip(formed, n_holdout))\n",
    "    table_1.to_csv('mann_Appendix_1_block_'+str(block)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = tops.Formation.value_counts(sort=True)\n",
    "fm_mapping = {1000:'Mannville Group', 2000:'t61', 3000:'t51', 4000:'t41', 5000:'t31', 6000:'Clearwater/Wabiskaw', 7000:'t21',\n",
    "              8000:'e20', 9000:'t15', 9500:'e14', 10000:'t11',11000:'t10.5', 12000:'e10', 13000:'McMurray Formation', \n",
    "              14000:'Paleozoic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(10,10))\n",
    "for i in enumerate(strat_order):\n",
    "    width = 1\n",
    "    height = 1\n",
    "    lims = (0, 10)\n",
    "    \n",
    "    ax1 = fig1.add_subplot(111, aspect='equal')\n",
    "    ax1.add_patch(\n",
    "        patches.Rectangle((0, i[0]*-1), width, height, color=colors_to_plot[str(i[1])]))\n",
    "    ax1.annotate(fm_mapping[i[1]], (1.5, -1*i[0]+0.25), fontsize=12)\n",
    "    ax1.annotate(vc[vc.index ==i[1]].values,  (6, -1*i[0]+0.25), fontsize=12)\n",
    "    plt.ylim(-59,1)\n",
    "    plt.xlim(lims)\n",
    "plt.tight_layout()\n",
    "plt.axis('off')\n",
    "#plt.savefig('mann_strat_column.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is output from the folds, compressed by hand in excel \n",
    "errors = pd.read_csv('mann_errors.csv') \n",
    "plt.rcParams.update({'font.size': 6})\n",
    "grid = plt.GridSpec(2, 1, wspace=0.4, hspace=0.1)\n",
    "fig = plt.figure(figsize=(2.5, 5))\n",
    "\n",
    "upper = plt.subplot(grid[0, :])\n",
    "\n",
    "for formation in strat_order:\n",
    "    subset = errors[errors.Formation == formation]\n",
    "    upper.scatter(subset['n_holdout']+subset['n_train'], subset['MAE'], \n",
    "                color=colors_to_plot[str(formation)], alpha=0.75, s=15)\n",
    "    upper.scatter(subset['n_holdout.1']+subset['n_train.1'], subset['MAE.1'],\n",
    "                color=colors_to_plot[str(formation)], alpha=0.75, s=15)\n",
    "    upper.scatter(subset['n_holdout.2']+subset['n_train.2'], subset['MAE.2'],\n",
    "                color=colors_to_plot[str(formation)], alpha=0.75, s=15)\n",
    "    up = upper.scatter(subset['n_holdout.3']+subset['n_train.3'], subset['MAE.3'],\n",
    "                color=colors_to_plot[str(formation)], alpha=0.75, s=15)\n",
    "upper.semilogx()\n",
    "upper.semilogy()\n",
    "upper.set_ylabel('MAE (m)', fontsize=6)\n",
    "upper.set_title('Error and Number of Picks', fontsize=6)\n",
    "\n",
    "#upper.set_xticklabels([10**1,10**2, 10**3],fontsize=6,)\n",
    "#upper.set_yticklabels([10^-1,10**0, 10**1, 10**2, 10**3],fontsize=6)#plt.yticks(fontsize=6)\n",
    "upper.set_xlim(100,5000)\n",
    "upper.set_ylim(1,110)\n",
    "\n",
    "\n",
    "lower = plt.subplot(grid[1, :])\n",
    "\n",
    "for formation in strat_order:\n",
    "    subset = errors[errors.Formation == formation]\n",
    "    lower.scatter(subset['n_holdout']+subset['n_train'], subset['RMSE'], \n",
    "                color=colors_to_plot[str(formation)], alpha=0.75, s=15)\n",
    "    lower.scatter(subset['n_holdout.1']+subset['n_train.1'], subset['RMSE.1'],\n",
    "                color=colors_to_plot[str(formation)], alpha=0.75, s=15)\n",
    "    lower.scatter(subset['n_holdout.2']+subset['n_train.2'], subset['RMSE.2'],\n",
    "                color=colors_to_plot[str(formation)], alpha=0.75, s=15)\n",
    "    lo = lower.scatter(subset['n_holdout.3']+subset['n_train.3'], subset['RMSE.3'],\n",
    "                color=colors_to_plot[str(formation)], alpha=0.75, s=15)\n",
    "lower.semilogx()\n",
    "lower.semilogy()\n",
    "lower.set_xlabel('Number of picks per top', fontsize=6)\n",
    "lower.set_ylabel('RMSE (m)', fontsize=6)\n",
    "#lower.set_title('Root Mean Squared Error per Formation', fontsize=6)\n",
    "#lower.set_xticklabels([10**1,10**2, 10**3],fontsize=6,)\n",
    "#lower.set_yticklabels([10^-1,10**0, 10**1, 10**2, 10**3],fontsize=6)#plt.yticks(fontsize=6)\n",
    "lower.set_xlim(100,5000)\n",
    "lower.set_ylim(1,110)\n",
    "\n",
    "\n",
    "plt.savefig('mann_MAE_formation.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error by well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDF = pd.concat(cross_validation_wells(tops, 86, L, its, 0.1))\n",
    "masterDF.to_csv('mann_error_map.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation with SOTA Green's Function spline interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import verde as vd\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = pd.read_csv(r\"mannville_cleaned.csv\", index_col=[0]) #read in the top data\n",
    "tops.dropna(inplace=True)\n",
    "tops = tops[tops.Quality >=0]\n",
    "tops.rename(columns={\"TVDSS\": \"SS\"}, inplace=True)\n",
    "well_locations = pd.read_csv(r'well_lat_lng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verde_error(location_dataframe, tops_df, grid_spacing, random_seed):\n",
    "    \"\"\"\n",
    "    Spatially interpolates between sample locations and measures error\n",
    "    using four-fold cross validation\n",
    "    :param location_dataframe: dataframe with locations\n",
    "    :param tops_df: dataframe with tops data\n",
    "    :param grid_spacing: grid square size in degrees\n",
    "    :param random_seed: random seed for reproducibility\n",
    "    :return: dictionary with MAE and RMSE for the 4 folds along with average\n",
    "    \"\"\"\n",
    "    data_dict={\"Top\":[], \"MAE 1\":[], \"MAE 2\":[], \"MAE 3\":[], \"MAE 4\":[], \"MAE avg\":[], \"RMSE 1\":[], \"RMSE 2\":[],\n",
    "              \"RMSE 3\":[], \"RMSE 4\":[], \"RMSE avg\":[]}\n",
    "    region = vd.get_region((location_dataframe.lng.values, location_dataframe.lat.values))\n",
    "    spacing = grid_spacing\n",
    "    cross_validator = KFold(n_splits=4, random_state=random_seed)\n",
    "    for formation in strat_order:\n",
    "        subset_DF = tops_df[tops_df.Formation == formation]\n",
    "        subset_DF = subset_DF.merge(location_dataframe[[\"lng\", \"lat\", \"SitID\"]], on=\"SitID\")\n",
    "        if subset_DF.shape[0] < 4:\n",
    "            data_dict['Top'].append(formation)\n",
    "            data_dict['MAE 1'].append(np.nan)\n",
    "            data_dict['MAE 2'].append(np.nan)\n",
    "            data_dict['MAE 3'].append(np.nan)\n",
    "            data_dict['MAE 4'].append(np.nan)\n",
    "            data_dict['RMSE 1'].append(np.nan)\n",
    "            data_dict['RMSE 2'].append(np.nan)\n",
    "            data_dict['RMSE 3'].append(np.nan)\n",
    "            data_dict['RMSE 4'].append(np.nan)\n",
    "            data_dict['MAE avg'].append(np.nan)\n",
    "            data_dict['RMSE avg'].append(np.nan)\n",
    "            \n",
    "        else:\n",
    "            coordinates = (subset_DF.lng.values, subset_DF.lat.values)\n",
    "            spline = vd.Spline()\n",
    "            RMSE = vd.cross_val_score(spline, coordinates, subset_DF.SS.values, \n",
    "                       cv=cross_validator, scoring=\"neg_root_mean_squared_error\")*-1\n",
    "            MAE = vd.cross_val_score(spline, coordinates, subset_DF.SS.values, \n",
    "                       cv=cross_validator, scoring=\"neg_mean_absolute_error\")*-1\n",
    "            data_dict['Top'].append(formation)\n",
    "            data_dict['MAE 1'].append(MAE[0])\n",
    "            data_dict['MAE 2'].append(MAE[1])\n",
    "            data_dict['MAE 3'].append(MAE[2])\n",
    "            data_dict['MAE 4'].append(MAE[3])\n",
    "            data_dict['RMSE 1'].append(RMSE[0])\n",
    "            data_dict['RMSE 2'].append(RMSE[1])\n",
    "            data_dict['RMSE 3'].append(RMSE[2])\n",
    "            data_dict['RMSE 4'].append(RMSE[3])\n",
    "            data_dict['MAE avg'].append(np.mean(MAE))\n",
    "            data_dict['RMSE avg'].append(np.mean(RMSE))\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(verde_error(well_locations, tops, 0.01, 86))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verde_spatial_error(location_dataframe, tops_df, block_size, random_seed):\n",
    "    \"\"\"\n",
    "    Spatially interpolates between sample locations and measures error\n",
    "    using blocked spatial four-fold cross validation\n",
    "    :param location_dataframe: dataframe with locations\n",
    "    :param tops_df: dataframe with tops data\n",
    "    :param block_size: size of the spatial block \n",
    "    :param random_seed: random seed for reproducibility\n",
    "    :return: dictionary with MAE and RMSE for the 4 folds along with average\n",
    "    \"\"\"\n",
    "    data_dict={\"Top\":[], \"MAE 1\":[], \"MAE 2\":[], \"MAE 3\":[], \"MAE 4\":[], \"MAE avg\":[], \"RMSE 1\":[], \"RMSE 2\":[],\n",
    "              \"RMSE 3\":[], \"RMSE 4\":[], \"RMSE avg\":[]}\n",
    "    region = vd.get_region((location_dataframe.lng.values, location_dataframe.lat.values))\n",
    "    cross_validator = vd.BlockKFold(spacing=block_size, n_splits=4, random_state=random_seed, shuffle=True,\n",
    "                                   balance=True)\n",
    "    for formation in strat_order:\n",
    "        subset_DF = tops_df[tops_df.Formation == formation]\n",
    "        subset_DF = subset_DF.merge(location_dataframe[[\"lat\", \"lng\", \"SitID\"]], on=\"SitID\")\n",
    "        if subset_DF.shape[0] < 4:\n",
    "            data_dict['Top'].append(formation)\n",
    "            data_dict['MAE 1'].append(np.nan)\n",
    "            data_dict['MAE 2'].append(np.nan)\n",
    "            data_dict['MAE 3'].append(np.nan)\n",
    "            data_dict['MAE 4'].append(np.nan)\n",
    "            data_dict['RMSE 1'].append(np.nan)\n",
    "            data_dict['RMSE 2'].append(np.nan)\n",
    "            data_dict['RMSE 3'].append(np.nan)\n",
    "            data_dict['RMSE 4'].append(np.nan)\n",
    "            data_dict['MAE avg'].append(np.nan)\n",
    "            data_dict['RMSE avg'].append(np.nan)\n",
    "            \n",
    "        else:\n",
    "            coordinates = (subset_DF.lng.values, subset_DF.lat.values)\n",
    "            spline = vd.Spline()\n",
    "            RMSE = vd.cross_val_score(spline, coordinates, subset_DF.SS.values, \n",
    "                       cv=cross_validator, scoring=\"neg_root_mean_squared_error\")*-1\n",
    "            CMAE = vd.cross_val_score(spline, coordinates, subset_DF.SS.values, \n",
    "                       cv=cross_validator, scoring=\"neg_mean_absolute_error\")*-1\n",
    "            data_dict['Top'].append(formation)\n",
    "            data_dict['MAE 1'].append(CMAE[0])\n",
    "            data_dict['MAE 2'].append(CMAE[1])\n",
    "            data_dict['MAE 3'].append(CMAE[2])\n",
    "            data_dict['MAE 4'].append(CMAE[3])\n",
    "            data_dict['RMSE 1'].append(RMSE[0])\n",
    "            data_dict['RMSE 2'].append(RMSE[1])\n",
    "            data_dict['RMSE 3'].append(RMSE[2])\n",
    "            data_dict['RMSE 4'].append(RMSE[3])\n",
    "            data_dict['MAE avg'].append(np.mean(CMAE))\n",
    "            data_dict['RMSE avg'].append(np.mean(RMSE))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(verde_spatial_error(well_locations, tops, 0.1, 86))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recsys_spatial_error(location_dataframe, tops_df, block_size, random_seed, latent, iterations, reg):\n",
    "    \"\"\"\n",
    "    Runs ALS recsys and measures error\n",
    "    using blocked spatial four-fold cross validation\n",
    "    :param location_dataframe: dataframe with locations\n",
    "    :param tops_df: dataframe with tops data\n",
    "    :param block_size: size of the spatial block \n",
    "    :param random_seed: random seed for reproducibility\n",
    "    :param latent: number of latent factors for RecSys\n",
    "    :param iterations: number of iterations for RecSys\n",
    "    :param reg: L2 regularization value\n",
    "    :return: dictionary with MAE and RMSE for the 4 folds along with average\n",
    "    \"\"\"\n",
    "    ss_df = tops_df.merge(location_dataframe[[\"lat\", \"lng\", \"SitID\"]], on=\"SitID\")\n",
    "    coordinates = (ss_df.lng.values, ss_df.lat.values)\n",
    "    kfold = vd.BlockKFold(spacing=block_size, n_splits=4, random_state=random_seed, shuffle=True, balance=True)\n",
    "    feature_matrix = np.transpose(coordinates)\n",
    "    data_dict={\"Top\":[], \"N_train 1\":[], \"N_test 1\":[],\"N_train 2\":[], \"N_test 2\":[],\"N_train 3\":[], \n",
    "           \"N_test 3\":[],\"N_train 4\":[], \"N_test 4\":[], \"MAE 1\":[], \"MAE 2\":[], \"MAE 3\":[], \n",
    "           \"MAE 4\":[], \"MAE avg\":[], \"RMSE 1\":[], \"RMSE 2\":[],\n",
    "              \"RMSE 3\":[], \"RMSE 4\":[], \"RMSE avg\":[]}\n",
    "    for formation in strat_order:\n",
    "        full = []\n",
    "        CV_MAE = []\n",
    "        CV_MSE = []\n",
    "        n_train = []\n",
    "        n_test = []\n",
    "        for train, test in kfold.split(feature_matrix):\n",
    "            test_set = ss_df.loc[test]\n",
    "            validate = test_set[test_set.Formation == formation]\n",
    "            validate_index = validate.index.values\n",
    "            main_group = ss_df.drop(validate_index)\n",
    "\n",
    "            D_df = main_group.pivot_table(\"SS\", \"Formation\", \"API\").fillna(\n",
    "                0\n",
    "            )  # pivot table to move into sparse matrix land\n",
    "            R = D_df.values\n",
    "            A = binarize(R)\n",
    "\n",
    "            U, Vt = runALS(R, A, latent, iterations, reg)\n",
    "\n",
    "            recommendations = np.dot(U, Vt)  # get the recommendations\n",
    "\n",
    "            recsys = pd.DataFrame(\n",
    "                data=recommendations[0:, 0:], index=D_df.index, columns=D_df.columns\n",
    "            )  # results\n",
    "\n",
    "            newDF = recsys.T\n",
    "            newDF.reset_index(inplace=True)\n",
    "\n",
    "            flat_preds = pd.DataFrame(recsys.unstack()).reset_index()\n",
    "\n",
    "            new_df = pd.merge(\n",
    "                validate,\n",
    "                flat_preds,\n",
    "                how=\"left\",\n",
    "                left_on=[\"API\", \"Formation\"],\n",
    "                right_on=[\"API\", \"Formation\"],\n",
    "            )\n",
    "            new_df.rename(columns={0: \"SS_pred\"}, inplace=True)\n",
    "            cleanDF = new_df.dropna()\n",
    "            cleanDF[\"signed_error\"] = cleanDF[\"SS\"] - cleanDF[\"SS_pred\"]\n",
    "\n",
    "            full.append(cleanDF.merge(location_dataframe[[\"lat\", \"lng\", \"API\"]], on=\"API\"))\n",
    "            CV_MAE.append(MAE(cleanDF.SS.values - ssmin, cleanDF.SS_pred.values - ssmin))\n",
    "            CV_MSE.append(\n",
    "                np.sqrt(MSE(cleanDF.SS.values - ssmin, cleanDF.SS_pred.values - ssmin))\n",
    "            )\n",
    "            n_train.append(len(main_group[main_group.Formation == \"F2WC\"]))\n",
    "            n_test.append(len(validate))\n",
    "        data_dict['Top'].append(formation)\n",
    "        data_dict['N_train 1'].append(n_train[0])\n",
    "        data_dict['N_train 2'].append(n_train[1])\n",
    "        data_dict['N_train 3'].append(n_train[2])\n",
    "        data_dict['N_train 4'].append(n_train[3])\n",
    "        data_dict['N_test 1'].append(n_test[0])\n",
    "        data_dict['N_test 2'].append(n_test[1])\n",
    "        data_dict['N_test 3'].append(n_test[2])\n",
    "        data_dict['N_test 4'].append(n_test[3])\n",
    "\n",
    "        data_dict['MAE 1'].append(CV_MAE[0])\n",
    "        data_dict['MAE 2'].append(CV_MAE[1])\n",
    "        data_dict['MAE 3'].append(CV_MAE[2])\n",
    "        data_dict['MAE 4'].append(CV_MAE[3])\n",
    "        data_dict['RMSE 1'].append(CV_MSE[0])\n",
    "        data_dict['RMSE 2'].append(CV_MSE[1])\n",
    "        data_dict['RMSE 3'].append(CV_MSE[2])\n",
    "        data_dict['RMSE 4'].append(CV_MSE[3])\n",
    "        data_dict['MAE avg'].append(np.mean(CV_MAE))\n",
    "        data_dict['RMSE avg'].append(np.mean(CV_MSE))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df = tops.merge(well_locations[[\"lat\", \"lng\", \"SitID\"]], on=\"SitID\")\n",
    "coordinates = (ss_df.lng.values, ss_df.lat.values)\n",
    "kfold = vd.BlockKFold(spacing=0.1, n_splits=4, random_state=86, shuffle=True, balance=True)\n",
    "feature_matrix = np.transpose(coordinates)\n",
    "for train, test in kfold.split(feature_matrix):\n",
    "    print(ss_df.loc[test].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(ss_df.loc[train].lng, ss_df.loc[train].lat, c='k', alpha=0.2)\n",
    "plt.scatter(ss_df.loc[test].lng, ss_df.loc[test].lat, c='r', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recsys_spatial_error(location_dataframe, tops_df, block_size, random_seed, latent, iterations, reg):\n",
    "    \"\"\"\n",
    "    Runs ALS recsys and measures error\n",
    "    using blocked spatial four-fold cross validation\n",
    "    :param location_dataframe: dataframe with locations\n",
    "    :param tops_df: dataframe with tops data\n",
    "    :param block_size: size of the spatial block \n",
    "    :param random_seed: random seed for reproducibility\n",
    "    :param latent: number of latent factors for RecSys\n",
    "    :param iterations: number of iterations for RecSys\n",
    "    :param reg: L2 regularization value\n",
    "    :return: dictionary with MAE and RMSE for the 4 folds along with average\n",
    "    \"\"\"\n",
    "    ss_df = tops_df.merge(location_dataframe[[\"lat\", \"lng\", \"SitID\"]], on=\"SitID\")\n",
    "    coordinates = (ss_df.lng.values, ss_df.lat.values)\n",
    "    kfold = vd.BlockKFold(spacing=block_size, n_splits=4, random_state=random_seed, shuffle=True, balance=True)\n",
    "    feature_matrix = np.transpose(coordinates)\n",
    "    data_dict={\"Top\":[], \"N_train 1\":[], \"N_test 1\":[],\"N_train 2\":[], \"N_test 2\":[],\"N_train 3\":[], \n",
    "           \"N_test 3\":[],\"N_train 4\":[], \"N_test 4\":[], \"MAE 1\":[], \"MAE 2\":[], \"MAE 3\":[], \n",
    "           \"MAE 4\":[], \"MAE avg\":[], \"RMSE 1\":[], \"RMSE 2\":[],\n",
    "              \"RMSE 3\":[], \"RMSE 4\":[], \"RMSE avg\":[]}\n",
    "    for formation in strat_order:\n",
    "        if len(ss_df[ss_df.Formation == formation]) < 1:\n",
    "            print(f\"Not enough picks to proceed... Skipping {formation}\")\n",
    "        else:\n",
    "                  \n",
    "            print(f\"Starting with {formation}\")\n",
    "            full = []\n",
    "            CV_MAE = []\n",
    "            CV_MSE = []\n",
    "            n_train = []\n",
    "            n_test = []\n",
    "            for train, test in kfold.split(feature_matrix):\n",
    "                test_set = ss_df.loc[test]\n",
    "                validate = test_set[test_set.Formation == formation]\n",
    "                validate_index = validate.index.values\n",
    "                main_group = ss_df.drop(validate_index)\n",
    "\n",
    "                D_df = main_group.pivot_table(\"SS\", \"Formation\", \"SitID\").fillna(\n",
    "                    0\n",
    "                )  # pivot table to move into sparse matrix land\n",
    "                R = D_df.values\n",
    "                A = binarize(R)\n",
    "\n",
    "                U, Vt = runALS(R, A, latent, iterations, reg)\n",
    "\n",
    "                recommendations = np.dot(U, Vt)  # get the recommendations\n",
    "\n",
    "                recsys = pd.DataFrame(\n",
    "                    data=recommendations[0:, 0:], index=D_df.index, columns=D_df.columns\n",
    "                )  # results\n",
    "\n",
    "                newDF = recsys.T\n",
    "                newDF.reset_index(inplace=True)\n",
    "\n",
    "                flat_preds = pd.DataFrame(recsys.unstack()).reset_index()\n",
    "\n",
    "                new_df = pd.merge(\n",
    "                    validate,\n",
    "                    flat_preds,\n",
    "                    how=\"left\",\n",
    "                    left_on=[\"SitID\", \"Formation\"],\n",
    "                    right_on=[\"SitID\", \"Formation\"],\n",
    "                )\n",
    "                new_df.rename(columns={0: \"SS_pred\"}, inplace=True)\n",
    "                cleanDF = new_df.dropna()\n",
    "                cleanDF[\"signed_error\"] = cleanDF[\"SS\"] - cleanDF[\"SS_pred\"]\n",
    "\n",
    "                full.append(cleanDF.merge(location_dataframe[[\"lat\", \"lng\", \"SitID\"]], on=\"SitID\"))\n",
    "                CV_MAE.append(MAE(cleanDF.SS.values, cleanDF.SS_pred.values))\n",
    "                CV_MSE.append(\n",
    "                    np.sqrt(MSE(cleanDF.SS.values, cleanDF.SS_pred.values))\n",
    "                )\n",
    "                n_train.append(len(main_group[main_group.Formation == formation]))\n",
    "                n_test.append(len(validate))\n",
    "        data_dict['Top'].append(formation)\n",
    "        data_dict['N_train 1'].append(n_train[0])\n",
    "        data_dict['N_train 2'].append(n_train[1])\n",
    "        data_dict['N_train 3'].append(n_train[2])\n",
    "        data_dict['N_train 4'].append(n_train[3])\n",
    "        data_dict['N_test 1'].append(n_test[0])\n",
    "        data_dict['N_test 2'].append(n_test[1])\n",
    "        data_dict['N_test 3'].append(n_test[2])\n",
    "        data_dict['N_test 4'].append(n_test[3])\n",
    "\n",
    "        data_dict['MAE 1'].append(CV_MAE[0])\n",
    "        data_dict['MAE 2'].append(CV_MAE[1])\n",
    "        data_dict['MAE 3'].append(CV_MAE[2])\n",
    "        data_dict['MAE 4'].append(CV_MAE[3])\n",
    "        data_dict['RMSE 1'].append(CV_MSE[0])\n",
    "        data_dict['RMSE 2'].append(CV_MSE[1])\n",
    "        data_dict['RMSE 3'].append(CV_MSE[2])\n",
    "        data_dict['RMSE 4'].append(CV_MSE[3])\n",
    "        data_dict['MAE avg'].append(np.mean(CV_MAE))\n",
    "        data_dict['RMSE avg'].append(np.mean(CV_MSE))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(recsys_spatial_error(well_locations, tops, 0.1, 86, 3, 100, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
